{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Residual Networks (ResNet50) using Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9G8ggYKV-gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at_d0QmFV_r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf sample_data\n",
        "! cp -R /content/drive/My\\ Drive/Datasets/Kannada-MNIST /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRc4bmHTWFRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBD7h6KyWfja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, filter_size, n_filters):\n",
        "    '''\n",
        "        X: input to identity block\n",
        "        n_filters: list of number of filters in each layer\n",
        "        filter_size: size of filters (window size) to be used in middle convolution layer\n",
        "    '''\n",
        "    \n",
        "    f1, f2, f3 = n_filters\n",
        "    X_shortcut = X # Storing input to add it later\n",
        "    \n",
        "    # First layer of identity block\n",
        "    X = Conv2D(filters = f1, kernel_size=(1, 1), strides=(1, 1), padding=\"valid\")(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second layer of identity block\n",
        "    X = Conv2D(filters = f2, kernel_size=(filter_size, filter_size), strides=(1, 1), padding=\"same\")(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Third layer\n",
        "    X = Conv2D(filters = f3, kernel_size=(1, 1), strides=(1, 1), padding = \"valid\")(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    \n",
        "    # Now add the input to above X before applying relu\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb_HijPNlM1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block(X, n_filters, filter_size, stride = 2):\n",
        "    '''\n",
        "        X: input to identity block\n",
        "        n_filters: list of number of filters in each layer\n",
        "        filter_size: size of filters (window size) to be used in middle convolution layer\n",
        "        stride: stride to be used in short cut path convolution to reduce dimensions\n",
        "    '''\n",
        "    \n",
        "    f1, f2, f3 = n_filters\n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First block\n",
        "    X = Conv2D(filters = f1, kernel_size = (1, 1), strides = (stride, stride), padding = 'valid')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second block\n",
        "    X = Conv2D(filters = f2, kernel_size = (filter_size, filter_size), strides = (1, 1), padding = 'same')(X)\n",
        "    X = BatchNormalization(axis = 3) (X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Third block\n",
        "    X = Conv2D(filters = f3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Shortcut convolution\n",
        "    X_shortcut = Conv2D(filters = f3, kernel_size = (1, 1), strides = (stride, stride), padding='valid')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "    \n",
        "    \n",
        "    # Add shortcut path to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iMQThx_oE9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet50(input_shape = (28, 28, 1), classes = 10):\n",
        "    \n",
        "    # First define input with given shape\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    X = Conv2D(filters = 64, kernel_size = (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed = 0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides = (2, 2))(X)\n",
        "    \n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, filter_size = 3, n_filters = [64, 64, 256], stride = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, filter_size = 3, n_filters = [128,128,512], stride = 2)\n",
        "    X = identity_block(X, 3, [128,128,512])\n",
        "    X = identity_block(X, 3, [128,128,512])\n",
        "    X = identity_block(X, 3, [128,128,512])\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, filter_size = 3, n_filters = [256, 256, 1024], stride = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, filter_size = 3, n_filters = [512, 512, 2048], stride = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    X = AveragePooling2D((2, 2), padding = 'same')(X)\n",
        "    \n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGlal2PIvRnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet50(input_shape = (28, 28, 1), classes = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whyrGwTHvWPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1pOQyFZxjXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constants\n",
        "IMG_SIZE = 28\n",
        "N_CHANNELS = 1 # because gray scale images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwM0phuhyro7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('Kannada-MNIST/train.csv')\n",
        "test_df = pd.read_csv('Kannada-MNIST/Dig-MNIST.csv')\n",
        "pred_df = pd.read_csv('Kannada-MNIST/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgZhGj5Pyr6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.append(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk5XMBw3y0OE",
        "colab_type": "code",
        "outputId": "acd1a17b-ca53-44fa-bd5b-39467e205841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
              "0      0       0       0       0  ...         0         0         0         0\n",
              "1      1       0       0       0  ...         0         0         0         0\n",
              "2      2       0       0       0  ...         0         0         0         0\n",
              "3      3       0       0       0  ...         0         0         0         0\n",
              "4      4       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odsTebnby1_E",
        "colab_type": "code",
        "outputId": "c83f4d4b-7e54-4e13-84a6-1d98e3f1f7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (f'Training set: {train_df.shape}')\n",
        "print (f'To be Predicted: {pred_df.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (70240, 785)\n",
            "To be Predicted: (5000, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NggkRTsdzA49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train_df.drop(['label'], axis = 1)\n",
        "Y_train = train_df['label']\n",
        "X_pred = pred_df.drop(['id'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iydKPrWpy5qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8unpN3Uy8ZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, X_pred = X_train.apply(lambda x: x/255), X_test.apply(lambda x: x/255), X_pred.apply(lambda x: x/255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWEzlhwRzETu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train, Y_test = pd.get_dummies(Y_train), pd.get_dummies(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjTi8NEzHpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
        "X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78eVZ5uzJgq",
        "colab_type": "code",
        "outputId": "711b3969-c892-46a0-c0d1-2c7444e6dd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (f'Training images: {X_train.shape}')\n",
        "print (f'Testing images: {X_test.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training images: (63216, 28, 28, 1)\n",
            "Testing images: (7024, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0T8qLBDzLMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = Y_train.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPTuPuVAzNJV",
        "colab_type": "code",
        "outputId": "2a61a336-a459-4e7e-e594-108e61e8ac59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=4)\n",
        "count=0\n",
        "for row in ax:\n",
        "    for col in row:\n",
        "        col.set_title(np.argmax(Y_train[count, :]))\n",
        "        col.imshow(X_train[count, :, :, 0])\n",
        "        count += 1\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEICAYAAACZJtWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wbfNI/W8wI5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXex-y-WzPB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.15, # Randomly zoom image \n",
        "        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "# This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GD-YlVxzhbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDGdbJ1SzX4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=32\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PONZ1fmzbCO",
        "colab_type": "code",
        "outputId": "ab8c57f6-a488-4165-89e7-36f29568e815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_test,Y_test),\n",
        "                              steps_per_epoch=X_train.shape[0] // batch_size, \n",
        "                              callbacks=[learning_rate_reduction])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1975/1975 [==============================] - 432s 219ms/step - loss: 1.5765 - acc: 0.6080 - val_loss: 1.6677 - val_acc: 0.5625\n",
            "Epoch 2/5\n",
            "   1/1975 [..............................] - ETA: 6:04 - loss: 1.3637 - acc: 0.6250"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1379: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_accuracy` which is not available. Available metrics are: val_loss,val_acc,loss,acc,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1975/1975 [==============================] - 411s 208ms/step - loss: 0.7742 - acc: 0.7958 - val_loss: 0.4898 - val_acc: 0.8494\n",
            "Epoch 3/5\n",
            "1975/1975 [==============================] - 411s 208ms/step - loss: 0.9542 - acc: 0.7816 - val_loss: 9.2491 - val_acc: 0.2017\n",
            "Epoch 4/5\n",
            "1975/1975 [==============================] - 414s 209ms/step - loss: 0.6225 - acc: 0.8572 - val_loss: 0.1827 - val_acc: 0.9449\n",
            "Epoch 5/5\n",
            " 715/1975 [=========>....................] - ETA: 4:17 - loss: 1.0389 - acc: 0.7890"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGtO3RW9zc9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "def PlotLoss(his, epoch):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(0, epoch), his.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(np.arange(0, epoch), his.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n",
        "def PlotAcc(his, epoch):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(0, epoch), his.history[\"accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(np.arange(0, epoch), his.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlF7-oo0lJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PlotLoss(history, epochs)\n",
        "PlotAcc(history, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BHzSRr31DQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQK5jEVN0nFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(f'model.h5')\n",
        "with open(f'his.pkl', 'wb') as output:\n",
        "    pickle.dump(histories[model_type], output, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brq5rmoX0-Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l51Z0pYL1HCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}